{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Extract features from a penultimate layer in Emotion English DistilRoBERTa-base model"
      ],
      "metadata": {
        "id": "TgE9B3kso7fQ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkIt7_lKp0h-"
      },
      "source": [
        "# install the transformers library\n",
        "!pip install transformers\n",
        "\n",
        "# import required packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# work with cuda\n",
        "device = torch.device('cuda')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZNal9hXp6wm"
      },
      "source": [
        "# load tokenizer and model\n",
        "model_name = \"j-hartmann/emotion-english-distilroberta-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparation of dataset"
      ],
      "metadata": {
        "id": "UArW7snAsKew"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHEzNB5Up5Yc"
      },
      "source": [
        "# create list of texts\n",
        "pred_texts = ['I like that', 'That is annoying', 'This is great!', 'WouldnÂ´t recommend it.']"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_features(name):\n",
        "    def hook(model, input, output):\n",
        "        features[name] = output.detach()\n",
        "    return hook"
      ],
      "metadata": {
        "id": "17uoYUFSfDHc"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.classifier.dense.register_forward_hook(get_features('feats'))"
      ],
      "metadata": {
        "id": "DgVs2qkWfJJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract features from penultimate layer"
      ],
      "metadata": {
        "id": "GBXclXZitsg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# placeholders\n",
        "PREDICTIONS = []\n",
        "FEATS = []\n",
        "\n",
        "# placeholder for batch features\n",
        "features = {}\n",
        "\n",
        "for idx, inputs in enumerate(pred_texts):\n",
        "     \n",
        "    inputs = tokenizer(inputs, return_tensors=\"pt\")\n",
        "    inputs = inputs.to(device)\n",
        "\n",
        "    predictions=model(**inputs).logits\n",
        "\n",
        "    PREDICTIONS.append(predictions.detach().cpu().numpy())\n",
        "    FEATS.append(features['feats'].cpu().numpy())\n",
        "       "
      ],
      "metadata": {
        "id": "LEyuTKUp5PW3"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect features\n",
        "\n",
        "PREDICTIONS = np.concatenate(PREDICTIONS)\n",
        "FEATS = np.concatenate(FEATS)\n",
        "\n",
        "print('preds shape:', PREDICTIONS.shape)\n",
        "print('feats shape:', FEATS.shape)"
      ],
      "metadata": {
        "id": "_ybn8Qi74l9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize results\n",
        "\n",
        "PREDICTIONS = (np.exp(PREDICTIONS)/np.exp(PREDICTIONS).sum(-1,keepdims=True))\n",
        "\n",
        "FEATS = (np.exp(FEATS)/np.exp(FEATS).sum(-1,keepdims=True))\n"
      ],
      "metadata": {
        "id": "7cpGBdNW2rXP"
      },
      "execution_count": 145,
      "outputs": []
    }
  ]
}
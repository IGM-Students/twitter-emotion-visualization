{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install aiohttp==3.7.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqLX3eSzNvHQ",
        "outputId": "adeaad55-40dd-4ce0-c21e-89936b688953"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting aiohttp==3.7.0\n",
            "  Downloading aiohttp-3.7.0-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp==3.7.0) (6.0.3)\n",
            "Collecting async-timeout<4.0,>=3.0\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp==3.7.0) (22.1.0)\n",
            "Requirement already satisfied: chardet<4.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp==3.7.0) (3.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp==3.7.0) (1.8.2)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.8/dist-packages (from yarl<2.0,>=1.0->aiohttp==3.7.0) (2.10)\n",
            "Installing collected packages: async-timeout, aiohttp\n",
            "  Attempting uninstall: async-timeout\n",
            "    Found existing installation: async-timeout 4.0.2\n",
            "    Uninstalling async-timeout-4.0.2:\n",
            "      Successfully uninstalled async-timeout-4.0.2\n",
            "  Attempting uninstall: aiohttp\n",
            "    Found existing installation: aiohttp 3.8.3\n",
            "    Uninstalling aiohttp-3.8.3:\n",
            "      Successfully uninstalled aiohttp-3.8.3\n",
            "Successfully installed aiohttp-3.7.0 async-timeout-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nest_asyncio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "by7GUpt-N-EC",
        "outputId": "00d1d30f-7dca-48f6-f07e-3cb4d1a8b503"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nest_asyncio\n",
            "  Downloading nest_asyncio-1.5.6-py3-none-any.whl (5.2 kB)\n",
            "Installing collected packages: nest-asyncio\n",
            "Successfully installed nest-asyncio-1.5.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJyjIwyWMH9T",
        "outputId": "db996522-c9a7-4a2d-853c-508a684f4ba0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'twint'...\n",
            "remote: Enumerating objects: 47, done.\u001b[K\n",
            "remote: Counting objects: 100% (47/47), done.\u001b[K\n",
            "remote: Compressing objects: 100% (44/44), done.\u001b[K\n",
            "remote: Total 47 (delta 3), reused 14 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (47/47), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/twint\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 1)) (3.7.0)\n",
            "Collecting aiodns\n",
            "  Downloading aiodns-3.0.0-py3-none-any.whl (5.0 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 3)) (4.6.3)\n",
            "Collecting cchardet\n",
            "  Downloading cchardet-2.1.7-cp38-cp38-manylinux2010_x86_64.whl (265 kB)\n",
            "\u001b[K     |████████████████████████████████| 265 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting dataclasses\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Collecting elasticsearch\n",
            "  Downloading elasticsearch-8.5.3-py3-none-any.whl (385 kB)\n",
            "\u001b[K     |████████████████████████████████| 385 kB 42.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pysocks in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 7)) (1.7.1)\n",
            "Requirement already satisfied: pandas>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 8)) (1.3.5)\n",
            "Collecting aiohttp_socks<=0.4.1\n",
            "  Downloading aiohttp_socks-0.4.1-py3-none-any.whl (17 kB)\n",
            "Collecting schedule\n",
            "  Downloading schedule-1.1.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: geopy in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 11)) (1.17.0)\n",
            "Collecting fake-useragent\n",
            "  Downloading fake_useragent-1.1.1-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 5.5 MB/s \n",
            "\u001b[?25hCollecting googletransx\n",
            "  Downloading googletransx-2.4.2.tar.gz (13 kB)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.23.0->-r requirements.txt (line 8)) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.23.0->-r requirements.txt (line 8)) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.23.0->-r requirements.txt (line 8)) (1.21.6)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp_socks<=0.4.1->-r requirements.txt (line 9)) (22.1.0)\n",
            "Requirement already satisfied: chardet<4.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->-r requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->-r requirements.txt (line 1)) (6.0.3)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->-r requirements.txt (line 1)) (3.0.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->-r requirements.txt (line 1)) (1.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas>=0.23.0->-r requirements.txt (line 8)) (1.15.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.8/dist-packages (from yarl<2.0,>=1.0->aiohttp->-r requirements.txt (line 1)) (2.10)\n",
            "Collecting pycares>=4.0.0\n",
            "  Downloading pycares-4.3.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
            "\u001b[K     |████████████████████████████████| 288 kB 46.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from pycares>=4.0.0->aiodns->-r requirements.txt (line 2)) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.5.0->pycares>=4.0.0->aiodns->-r requirements.txt (line 2)) (2.21)\n",
            "Collecting elastic-transport<9,>=8\n",
            "  Downloading elastic_transport-8.4.0-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 6.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from elastic-transport<9,>=8->elasticsearch->-r requirements.txt (line 6)) (2022.9.24)\n",
            "Collecting urllib3<2,>=1.26.2\n",
            "  Downloading urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 58.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: geographiclib<2,>=1.49 in /usr/local/lib/python3.8/dist-packages (from geopy->-r requirements.txt (line 11)) (1.52)\n",
            "Requirement already satisfied: importlib-resources>=5.0 in /usr/local/lib/python3.8/dist-packages (from fake-useragent->-r requirements.txt (line 12)) (5.10.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=5.0->fake-useragent->-r requirements.txt (line 12)) (3.11.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from googletransx->-r requirements.txt (line 13)) (2.23.0)\n",
            "Collecting requests\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->googletransx->-r requirements.txt (line 13)) (2.1.1)\n",
            "Building wheels for collected packages: twint, googletransx\n",
            "  Building wheel for twint (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for twint: filename=twint-2.1.21-py3-none-any.whl size=38870 sha256=ebc7e5923681592ed952afcae1fc6fd75db5c468bfdcf7cb6488f9dff6aac072\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-nsadf9a6/wheels/a9/70/09/93062f64eee119ab310a33bae425d98678f627d1dc1b667e8f\n",
            "  Building wheel for googletransx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletransx: filename=googletransx-2.4.2-py3-none-any.whl size=15968 sha256=248f67883c47137a78919ce0043fa50180c9a2595faa23fe6540caf8bb03ce48\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/54/98/96b62f08dd73eca3147e36b5099d77c3f8fedc5472bb488167\n",
            "Successfully built twint googletransx\n",
            "Installing collected packages: urllib3, requests, pycares, elastic-transport, schedule, googletransx, fake-useragent, elasticsearch, dataclasses, cchardet, aiohttp-socks, aiodns, twint\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "Successfully installed aiodns-3.0.0 aiohttp-socks-0.4.1 cchardet-2.1.7 dataclasses-0.6 elastic-transport-8.4.0 elasticsearch-8.5.3 fake-useragent-1.1.1 googletransx-2.4.2 pycares-4.3.0 requests-2.28.1 schedule-1.1.0 twint-2.1.21 urllib3-1.26.13\n"
          ]
        }
      ],
      "source": [
        "!git clone --depth=1 https://github.com/twintproject/twint.git\n",
        "!cd /content/twint && pip3 install . -r requirements.txt\n",
        "\n",
        "import twint\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install clean-text==0.6.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iv2SfIiQ4l7",
        "outputId": "14cce4c9-415f-4f74-ca7b-e4718251864b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting clean-text==0.6.0\n",
            "  Downloading clean_text-0.6.0-py3-none-any.whl (11 kB)\n",
            "Collecting emoji<2.0.0,>=1.0.0\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[K     |████████████████████████████████| 175 kB 4.9 MB/s \n",
            "\u001b[?25hCollecting ftfy<7.0,>=6.0\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.8/dist-packages (from ftfy<7.0,>=6.0->clean-text==0.6.0) (0.2.5)\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=200b9aded28de74772445b89209b8fead0f5b52c4489f801584131c5ce8fa0f8\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/8c/80/c3646df8201ba6f5070297fe3779a4b70265d0bfd961c15302\n",
            "Successfully built emoji\n",
            "Installing collected packages: ftfy, emoji, clean-text\n",
            "Successfully installed clean-text-0.6.0 emoji-1.7.0 ftfy-6.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cleantext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cLnGIiWQ3GM",
        "outputId": "f833e9a4-61f5-49f1-cead-26922f1f238e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "colors = [\n",
        "    \"#e60049\",\n",
        "    \"#0bb4ff\",\n",
        "    \"#50e991\",\n",
        "    \"#e6d800\",\n",
        "    \"#9b19f5\",\n",
        "    \"#ffa300\",\n",
        "    \"#dc0ab4\",\n",
        "    \"#b3d4ff\",\n",
        "    \"#00bfa0\"\n",
        "]\n",
        "\n",
        "hashtags = [\n",
        "    \"#joy\",\n",
        "    \"#sad\",\n",
        "    \"#happy\",\n",
        "    \"#fun\",\n",
        "    \"#angry\",\n",
        "    \"#bad\",\n",
        "    \"#disgusting\",\n",
        "    \"#fear\",\n",
        "    \"#surprise\"\n",
        "]\n",
        "\n",
        "twittsDfs = []\n",
        "\n",
        "for hashtag in hashtags:\n",
        "    print(hashtag)\n",
        "    \n",
        "    c = twint.Config()\n",
        "    c.Store_object = True\n",
        "    c.Search = hashtag\n",
        "    c.Limit = 5000\n",
        "    c.Since=\"2019-07-01\"\n",
        "    c.Lang = 'en'\n",
        "    c.Pandas = True\n",
        "    c.Pandas_clean = True\n",
        "    c.Hide_output = True\n",
        "    twint.run.Search(c)\n",
        "    twittsDfs.append(twint.storage.panda.Tweets_df)\n",
        "\n",
        "    print(twint.storage.panda.Tweets_df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOedROihMtbY",
        "outputId": "340b4fc9-bcdf-43e1-d072-be3202fa613e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#joy\n",
            "(5017, 38)\n",
            "#sad\n",
            "(5006, 38)\n",
            "#happy\n",
            "(5006, 38)\n",
            "#fun\n",
            "(5010, 38)\n",
            "#angry\n",
            "[!] No more data! Scraping will stop now.\n",
            "found 0 deleted tweets in this search.\n",
            "(361, 38)\n",
            "#bad\n",
            "[!] No more data! Scraping will stop now.\n",
            "found 0 deleted tweets in this search.\n",
            "(1279, 38)\n",
            "#disgusting\n",
            "[!] No more data! Scraping will stop now.\n",
            "found 0 deleted tweets in this search.\n",
            "(1461, 38)\n",
            "#fear\n",
            "[!] No more data! Scraping will stop now.\n",
            "found 0 deleted tweets in this search.\n",
            "(1691, 38)\n",
            "#surprise\n",
            "[!] No more data! Scraping will stop now.\n",
            "found 0 deleted tweets in this search.\n",
            "(939, 38)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "twitts = twittsDfs.copy()"
      ],
      "metadata": {
        "id": "er3Xpu0tdd4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Selecting English posts"
      ],
      "metadata": {
        "id": "MibIDsHVVjOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(twitts)):\n",
        "  twitts[i] = twitts[i][twitts[i].language == 'en']\n",
        "  print(twitts[i].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSdPqPNVPzeX",
        "outputId": "16ac78b2-d397-4807-c56c-8fd3cfe26951"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3464, 38)\n",
            "(3032, 38)\n",
            "(2767, 38)\n",
            "(3824, 38)\n",
            "(288, 38)\n",
            "(626, 38)\n",
            "(1185, 38)\n",
            "(1442, 38)\n",
            "(690, 38)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "twitts[1]['tweet'].head(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2fmcRCvQkW6",
        "outputId": "2fdb2c4d-8beb-4767-ea09-e4335f62e926"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     I’m going to miss flipping off 🖕🏼the bots 🤖. I...\n",
              "1                  And we still squeaked out a win #sad\n",
              "3     @dvngtp it's true, all he does these days is d...\n",
              "4       @JDCocchiarella She \"won\" via other means. #Sad\n",
              "5     @MercedesGlobal @TheWestBlock Every time I thi...\n",
              "7     @d33inhouston @stuarthazeldine @elonmusk I fin...\n",
              "8     Mail in voteing is the. Fraud !!!  Democrat sc...\n",
              "10    The white house administration is #sad shamefu...\n",
              "12    @KeithOlbermann You are such a #sad individual...\n",
              "15                               @PabloPNC Chach...#SAD\n",
              "18    @BenjaminPDixon You know, they are always look...\n",
              "19          $nottotoo #cashapp #sad #CristianoRonaldo 😂\n",
              "20    Stick figure pregnancy. #cartoonmania #animato...\n",
              "21    Even the artificial intelligence of openAI (ch...\n",
              "23    @FoxNews So, he thought killing himself was be...\n",
              "26    @adgirlMM @elonmusk And a #HorribleHuman to bo...\n",
              "27    blurry vent art. MADD, tobias,  matsu and a ca...\n",
              "28    first ever song snippet🖤#upcoming #artist #vir...\n",
              "29    Again I’m sorry and I’ll probably be gone for ...\n",
              "30    I’m sorry I know I just got back from a long h...\n",
              "Name: tweet, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(twitts[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umPBQ6owgKNE",
        "outputId": "ac0f622b-30cf-4dc1-e78b-eb6ba81a2422"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.frame.DataFrame"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(9):\n",
        "  for index, row in twitts[i].iterrows():\n",
        "      twitts[i].at[index,'tweet'] = cleantext.clean(row.tweet, no_emoji=True, no_urls=True)"
      ],
      "metadata": {
        "id": "56lTlSPUQdl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "twitts[1]['tweet'].head(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Pakwfc-OrjQ",
        "outputId": "e4038c6e-5aca-4e64-c68f-5b141d68fc4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     i'm going to miss flipping off the bots . i'm ...\n",
              "1                  and we still squeaked out a win #sad\n",
              "3     @dvngtp it's true, all he does these days is d...\n",
              "4       @jdcocchiarella she \"won\" via other means. #sad\n",
              "5     @mercedesglobal @thewestblock every time i thi...\n",
              "7     @d33inhouston @stuarthazeldine @elonmusk i fin...\n",
              "8     mail in voteing is the. fraud !!! democrat sca...\n",
              "10    the white house administration is #sad shamefu...\n",
              "12    @keitholbermann you are such a #sad individual...\n",
              "15                               @pablopnc chach...#sad\n",
              "18    @benjaminpdixon you know, they are always look...\n",
              "19            $nottotoo #cashapp #sad #cristianoronaldo\n",
              "20    stick figure pregnancy. #cartoonmania #animato...\n",
              "21    even the artificial intelligence of openai (ch...\n",
              "23    @foxnews so, he thought killing himself was be...\n",
              "26    @adgirlmm @elonmusk and a #horriblehuman to bo...\n",
              "27    blurry vent art. madd, tobias, matsu and a cat...\n",
              "28    first ever song snippet#upcoming #artist #vira...\n",
              "29    again i'm sorry and i'll probably be gone for ...\n",
              "30    i'm sorry i know i just got back from a long h...\n",
              "Name: tweet, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from google.colab import files\n",
        "\n",
        "header = ['text', 'hashtag', 'username']\n",
        "\n",
        "with open('tweets.csv', 'w', encoding='UTF8') as f:\n",
        "  writer = csv.writer(f, delimiter='\\\\')\n",
        "  writer.writerow(header)\n",
        "\n",
        "  for i in range(len(twitts)):\n",
        "    for j in range(len(twitts[i])):\n",
        "      row = [twitts[i].iloc[j].tweet, hashtags[i], twitts[i].iloc[j].username]\n",
        "\n",
        "      writer.writerow(row)\n",
        "\n",
        "\n",
        "files.download('tweets.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "hLfCe7mCWjEQ",
        "outputId": "48f61119-38f8-4717-f3cd-4231605b9b34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_3517329d-ae69-44c7-93be-df901ee67464\", \"tweets.csv\", 3271506)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}